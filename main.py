domains = [
    "energy",
    "information_technology",
    "health_care",
    "financial_services",
    "manufacturing_industrials",
    "consumer_goods_retail",
    "agriculture_food_systems",
    "transportation_logistics",
    "construction_real_estate",
    "media_telecom_entertainment"
]

companies = {
    "Energy": [
        "Saudi Aramco",
        "ExxonMobil",
        "Chevron",
        "Shell",
        "PetroChina",
        "BP",
        "TotalEnergies",
        "Equinor",
        "Eni",
        "ConocoPhillips",
        "Phillips 66",
        "Marathon Petroleum",
        "Valero Energy",
        "Reliance Industries",
        "NextEra Energy",
        "Duke Energy",
        "Enel",
        "Iberdrola",
        "China Shenhua Energy",
        "CNOOC",
        "Gazprom",
        "Rosneft",
        "Canadian Natural Resources",
        "Suncor Energy",
        "Occidental Petroleum"
    ],
    "Information Technology": [
        "Apple",
        "Microsoft",
        "Nvidia",
        "Alphabet",
        "Amazon",
        "Meta Platforms",
        "TSMC",
        "Samsung Electronics",
        "Broadcom",
        "Oracle",
        "SAP",
        "Salesforce",
        "Adobe",
        "Intel",
        "Cisco Systems",
        "ASML",
        "Accenture",
        "IBM",
        "Qualcomm",
        "Tencent Holdings",
        "Alibaba Group",
        "Infosys",
        "Tata Consultancy Services",
        "Sony Group",
        "Uber Technologies"
    ],
    "Health Care & Pharmaceuticals": [
        "Johnson & Johnson",
        "Pfizer",
        "Roche",
        "Novartis",
        "Merck & Co.",
        "AbbVie",
        "Sanofi",
        "Bristol Myers Squibb",
        "GSK",
        "AstraZeneca",
        "Eli Lilly and Company",
        "Novo Nordisk",
        "Bayer",
        "Amgen",
        "Takeda Pharmaceutical",
        "Gilead Sciences",
        "CSL Limited",
        "Biogen",
        "Regeneron Pharmaceuticals",
        "Moderna",
        "Vertex Pharmaceuticals",
        "Merck KGaA",
        "Daiichi Sankyo",
        "Boehringer Ingelheim",
        "Teva Pharmaceutical Industries"
    ],
    "Financial Services": [
        "JPMorgan Chase",
        "Bank of America",
        "Industrial and Commercial Bank of China",
        "China Construction Bank",
        "Agricultural Bank of China",
        "Wells Fargo",
        "Citigroup",
        "HSBC",
        "BNP Paribas",
        "Mitsubishi UFJ Financial Group",
        "Goldman Sachs",
        "Morgan Stanley",
        "UBS Group",
        "Allianz",
        "Ping An Insurance",
        "AXA",
        "MetLife",
        "American Express",
        "Visa",
        "Mastercard",
        "PayPal",
        "Berkshire Hathaway",
        "BlackRock",
        "Charles Schwab",
        "Banco Santander"
    ],
    "Manufacturing & Industrials": [
        "Toyota Motor Corporation",
        "Volkswagen Group",
        "Mercedes-Benz Group",
        "General Motors",
        "Ford Motor Company",
        "Honda Motor Co.",
        "BMW Group",
        "Stellantis",
        "Hyundai Motor Company",
        "SAIC Motor",
        "Tesla",
        "Boeing",
        "Airbus",
        "Caterpillar",
        "3M",
        "Siemens",
        "General Electric",
        "Honeywell",
        "Hitachi",
        "Mitsubishi Heavy Industries",
        "Panasonic",
        "Schneider Electric",
        "ABB",
        "Lockheed Martin",
        "Rolls-Royce Holdings"
    ],
    "Consumer Goods & Retail": [
        "Walmart",
        "Amazon",
        "Costco Wholesale",
        "The Home Depot",
        "Schwarz Group",
        "Aldi",
        "Tesco",
        "Carrefour",
        "Kroger",
        "Target",
        "Alibaba Group",
        "JD.com",
        "PDD Holdings",
        "Reliance Retail",
        "Inditex",
        "H&M",
        "Nike",
        "LVMH",
        "Procter & Gamble",
        "Unilever",
        "Nestlé",
        "PepsiCo",
        "The Coca-Cola Company",
        "Mondelez International",
        "Colgate-Palmolive"
    ],
    "Agriculture & Food Systems": [
        "Cargill",
        "Archer Daniels Midland",
        "Bayer Crop Science",
        "Nutrien",
        "Syngenta Group",
        "Corteva Agriscience",
        "Yara International",
        "Bunge Global",
        "Louis Dreyfus Company",
        "Olam Group",
        "CHS Inc.",
        "Deere & Company",
        "CNH Industrial",
        "Kubota",
        "Tyson Foods",
        "JBS",
        "Marfrig",
        "WH Group",
        "Fonterra",
        "Dairy Farmers of America",
        "Wilmar International",
        "KWS Saat",
        "Viterra",
        "Kerry Group",
        "Nestlé"
    ],
    "Transportation & Logistics": [
        "United Parcel Service",
        "FedEx",
        "Deutsche Post DHL Group",
        "Maersk",
        "CMA CGM",
        "MSC Mediterranean Shipping Company",
        "Hapag-Lloyd",
        "Kuehne + Nagel",
        "DSV",
        "DB Schenker",
        "CEVA Logistics",
        "Nippon Express",
        "C.H. Robinson",
        "XPO Logistics",
        "GXO Logistics",
        "J.B. Hunt Transport Services",
        "Union Pacific Railroad",
        "BNSF Railway",
        "Delta Air Lines",
        "American Airlines Group",
        "United Airlines Holdings",
        "Ryanair Holdings",
        "COSCO Shipping",
        "SF Holding",
        "HMM"
    ],
    "Construction & Real Estate": [
        "China State Construction Engineering Corporation",
        "China Railway Group",
        "China Railway Construction Corporation",
        "China Communications Construction Company",
        "VINCI",
        "ACS Group",
        "Bouygues",
        "Skanska",
        "HOCHTIEF",
        "Bechtel",
        "Larsen & Toubro",
        "Fluor Corporation",
        "Prologis",
        "American Tower Corporation",
        "Equinix",
        "Welltower",
        "Realty Income",
        "Simon Property Group",
        "Brookfield Asset Management",
        "Blackstone",
        "CBRE Group",
        "Mitsui Fudosan",
        "Sun Hung Kai Properties",
        "China Vanke",
        "Dalian Wanda Group"
    ],
    "Media, Telecom & Entertainment": [
        "Comcast",
        "The Walt Disney Company",
        "Warner Bros. Discovery",
        "Paramount Global",
        "Netflix",
        "Sony Group",
        "Tencent Holdings",
        "ByteDance",
        "Meta Platforms",
        "Alphabet",
        "Amazon",
        "Spotify Technology",
        "Electronic Arts",
        "Activision Blizzard",
        "Nintendo",
        "AT&T",
        "Verizon Communications",
        "Deutsche Telekom",
        "China Mobile",
        "China Telecom",
        "China Unicom",
        "Nippon Telegraph and Telephone",
        "Vodafone Group",
        "Telefónica",
        "Reliance Jio"
    ]
}

import requests
from bs4 import BeautifulSoup
import difflib
import re
from urllib.parse import urljoin
from collections import defaultdict
from typing import Dict, List, Any, Optional

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}


def normalize_name(name: str) -> str:
    return re.sub(r"[^a-z0-9]+", " ", name.lower()).strip()


def best_fuzzy_match(
    query: str, candidates: Dict[str, str], cutoff: float = 0.6
) -> Optional[str]:
    """
    Fuzzy-match query against candidate keys (company names).
    Returns the best-matching key or None.
    """
    if not candidates:
        return None
    names = list(candidates.keys())
    match = difflib.get_close_matches(query, names, n=1, cutoff=cutoff)
    return match[0] if match else None


# ---------------------- Screener.in (India) ----------------------


def build_screener_index(max_pages: int = 210) -> Dict[str, str]:
    """
    Build a name -> company_url index for all Indian companies on Screener.

    Walks the public "All listed companies" screen:
    https://www.screener.in/screens/357649/all-listed-companies/

    NOTE: This can take a while; consider adding sleeps or caching the result.
    """
    base = "https://www.screener.in"
    screen_url_tpl = (
        "https://www.screener.in/screens/357649/all-listed-companies/?page={page}"
    )

    index: Dict[str, str] = {}
    for page in range(1, max_pages + 1):
        url = screen_url_tpl.format(page=page)
        resp = requests.get(url, headers=HEADERS, timeout=30)
        if resp.status_code != 200:
            # Stop if we hit an error (could be last page or rate limiting)
            break

        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table")
        if not table:
            break

        page_added = 0
        for tr in table.find_all("tr"):
            a = tr.find("a", href=True)
            if not a:
                continue
            href = a["href"]
            if "/company/" not in href:
                continue

            raw_name = a.get_text(strip=True)
            if not raw_name:
                continue
            full_url = urljoin(base, href)
            index[raw_name] = full_url
            page_added += 1

        if page_added == 0:
            # probably no more data
            break

    return index


def fetch_reports_from_screener(
    company_name: str,
    screener_index: Dict[str, str],
    year: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    For a given company name, try to find its Screener page and
    extract Annual Report links (which typically point to BSE/NSE).

    Returns list of dicts: {"source", "matched_name", "year", "url"}.
    """
    results: List[Dict[str, Any]] = []

    match_name = best_fuzzy_match(company_name, screener_index)
    if not match_name:
        return results

    base_company_url = screener_index[match_name]
    # Prefer consolidated page (like /RELIANCE/consolidated/)
    if not base_company_url.rstrip("/").endswith("/consolidated"):
        if not base_company_url.endswith("/"):
            base_company_url += "/"
        base_company_url += "consolidated/"

    resp = requests.get(base_company_url, headers=HEADERS, timeout=30)
    if resp.status_code != 200:
        return results

    soup = BeautifulSoup(resp.text, "html.parser")
    for a in soup.find_all("a", href=True):
        href = a["href"]
        text = a.get_text(strip=True)
        if not href:
            continue

        # Screener Annual Report links for Indian companies usually
        # go to bseindia.com or nseindia.com and include "Financial Year".
        if ("bseindia.com" in href or "nseindia.com" in href) and (
            "Financial Year" in text or "Annual Report" in text
        ):
            year_found: Optional[int] = None
            m = re.search(r"(20\d{2})", text)
            if m:
                try:
                    year_found = int(m.group(1))
                except ValueError:
                    year_found = None

            if year is not None and year_found is not None and year_found != year:
                continue

            pdf_url = href if href.startswith("http") else urljoin(base_company_url, href)
            results.append(
                {
                    "source": "screener.in -> bseindia.com/nseindia.com",
                    "matched_name": match_name,
                    "year": year_found,
                    "url": pdf_url,
                }
            )

    # sort newest first
    results.sort(key=lambda x: x.get("year") or 0, reverse=True)
    return results


# ---------------------- AnnualReports.com (global) ----------------------


def build_annualreports_index() -> Dict[str, str]:
    """
    Build a name -> company_url index for AnnualReports.com.

    Scrapes https://www.annualreports.com/Companies and keeps only
    <a> tags whose href starts with /Company/.
    """
    base = "https://www.annualreports.com"
    url = f"{base}/Companies"
    resp = requests.get(url, headers=HEADERS, timeout=60)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")
    index: Dict[str, str] = {}
    for a in soup.find_all("a", href=True):
        href = a["href"]
        name = a.get_text(strip=True)
        if not name or not href.startswith("/Company/"):
            continue
        index[name] = urljoin(base, href)

    return index


def extract_reports_from_annualreports_page(
    soup: BeautifulSoup,
    year: Optional[int] = None,
    latest_only: bool = True,
) -> List[Dict[str, Any]]:
    """
    Given a company page soup from AnnualReports.com, extract report PDFs.

    It looks for anchors whose text is "View PDF" or "View Annual Report"
    and tries to infer the year from the surrounding text.
    """
    base = "https://www.annualreports.com"
    entries: List[Dict[str, Any]] = []

    for a in soup.find_all("a", href=True):
        text = a.get_text(strip=True)
        if text not in ("View PDF", "View Annual Report"):
            continue

        # Use the surrounding text to infer year
        context = a.find_parent().get_text(" ", strip=True)
        m = re.search(r"(19|20)\d{2}", context)
        year_found: Optional[int] = int(m.group(0)) if m else None

        if year is not None and year_found is not None and year_found != year:
            continue

        pdf_url = urljoin(base, a["href"])
        entries.append({"year": year_found, "url": pdf_url})

    # Deduplicate by (year, url)
    seen = set()
    unique_entries: List[Dict[str, Any]] = []
    for e in entries:
        key = (e["year"], e["url"])
        if key in seen:
            continue
        seen.add(key)
        unique_entries.append(e)

    unique_entries.sort(key=lambda x: x.get("year") or 0, reverse=True)
    if latest_only and unique_entries:
        return [unique_entries[0]]
    return unique_entries


def fetch_reports_from_annualreports(
    company_name: str,
    ar_index: Dict[str, str],
    year: Optional[int] = None,
    latest_only: bool = True,
) -> List[Dict[str, Any]]:
    """
    For a given company, find its AnnualReports.com page and extract PDF links.

    Returns list of dicts: {"source", "matched_name", "year", "url"}.
    """
    results: List[Dict[str, Any]] = []

    match_name = best_fuzzy_match(company_name, ar_index)
    if not match_name:
        return results

    company_url = ar_index[match_name]
    resp = requests.get(company_url, headers=HEADERS, timeout=60)
    if resp.status_code != 200:
        return results

    soup = BeautifulSoup(resp.text, "html.parser")
    entries = extract_reports_from_annualreports_page(
        soup, year=year, latest_only=latest_only
    )

    for e in entries:
        results.append(
            {
                "source": "annualreports.com",
                "matched_name": match_name,
                "year": e.get("year"),
                "url": e.get("url"),
            }
        )

    return results


# ---------------------- Orchestration ----------------------


def get_annual_report_links_for_companies(
    company_names: List[str],
    prefer_year: Optional[int] = None,
) -> Dict[str, List[Dict[str, Any]]]:
    """
    High-level helper: for each company name, try multiple providers
    (Screener for Indian names, AnnualReports.com for global names).

    Returns a dict:
        {
          "Reliance Industries Ltd": [
             {"source": "...", "matched_name": "...", "year": 2025, "url": "..."},
             ...
          ],
          ...
        }
    """
    results: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

    # Build indexes once
    try:
        screener_index = build_screener_index()
    except Exception:
        screener_index = {}

    try:
        ar_index = build_annualreports_index()
    except Exception:
        ar_index = {}

    for name in company_names:
        company_results: List[Dict[str, Any]] = []

        # Screener (mostly Indian equities)
        if screener_index:
            try:
                company_results.extend(
                    fetch_reports_from_screener(name, screener_index, year=prefer_year)
                )
            except Exception:
                pass

        # AnnualReports.com (global)
        if ar_index:
            try:
                company_results.extend(
                    fetch_reports_from_annualreports(
                        name, ar_index, year=prefer_year, latest_only=True
                    )
                )
            except Exception:
                pass

        results[name] = company_results

    return results

import tqdm
if __name__ == "__main__":
    for domain in tqdm.tqdm(companies.keys(), total = len(companies.keys())):
        comp = companies[domain]
        data = get_annual_report_links_for_companies(comp, prefer_year=None)

        # Pretty-print
        import json
        
        # save json file
        with open(f"{domain}_annual_report_links.json", "w") as f:
            f.write(json.dumps(data, indent=2))